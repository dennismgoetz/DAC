{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistische Regression CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dennis\\OneDrive\\Dokumente\\03_Master BAOR\\05_Kurse\\01_Business Analytics\\04_Data Analytics Challenge\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.getcwd()\n",
    "%cd \"C:\\Users\\Dennis\\OneDrive\\Dokumente\\03_Master BAOR\\05_Kurse\\01_Business Analytics\\04_Data Analytics Challenge\"\n",
    "\n",
    "ccdata = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Small dataset\n",
    "#ccdata = ccdata.iloc[:100000, :]\n",
    "\n",
    "# Balance of dataset target values\n",
    "display(ccdata.Class.value_counts())\n",
    "\n",
    "# Drop feature 'Time'\n",
    "ccdata.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 270099, 1: 467})\n",
      "\n",
      "Ratio of target value 1 in ccdata: 0.17 %\n",
      "Ratio of target value 1 in train_set: 0.17 %\n",
      "Ratio of target value 1 in test_set: 0.18 %\n"
     ]
    }
   ],
   "source": [
    "X = ccdata.iloc[:, :-1].to_numpy()\n",
    "y = ccdata.iloc[:, -1].to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.95, random_state=1, stratify=ccdata['Class'], shuffle=True)\n",
    "\n",
    "print('{}\\n'.format(Counter(y_train)))\n",
    "print('Ratio of target value 1 in ccdata: {:.2f} %'.format(100*np.sum(ccdata['Class'] == 1)/len(ccdata['Class'])))\n",
    "print('Ratio of target value 1 in train_set: {:.2f} %'.format(100*np.sum(y_train == 1)/len(y_train)))\n",
    "print('Ratio of target value 1 in test_set: {:.2f} %'.format(100*np.sum(y_test == 1)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 270099, 1: 270099})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Smote the dataset until its balanced\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=0, k_neighbors=5)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "Counter(y_res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "### L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean CV R^2 (term=0.001): 0.9468\n",
      "std CV R^2 (term=0.001): 0.0063 \n",
      "\n",
      "mean CV R^2 (term=0.01): 0.9709\n",
      "std CV R^2 (term=0.01): 0.0093 \n",
      "\n",
      "mean CV R^2 (term=0.1): 0.9786\n",
      "std CV R^2 (term=0.1): 0.0074 \n",
      "\n",
      "mean CV R^2 (term=1): 0.9765\n",
      "std CV R^2 (term=1): 0.0071 \n",
      "\n",
      "mean CV R^2 (term=10): 0.9761\n",
      "std CV R^2 (term=10): 0.0069 \n",
      "\n",
      "mean CV R^2 (term=100): 0.9761\n",
      "std CV R^2 (term=100): 0.0069 \n",
      "\n",
      "mean CV R^2 (term=1000): 0.9761\n",
      "std CV R^2 (term=1000): 0.0069 \n",
      "\n",
      "Best CV R^2: 0.9785761933580515\n",
      "Best expo: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "max_R2 = -np.inf\n",
    "best_expo = None\n",
    "reg_terms = [0.1, 1, 10]\n",
    "reg_terms2 = [10**i for i in [-3, -2, -1, 0, 1, 2, 3]] #[0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for term in reg_terms2: #find the optimal hyperparameter for C\n",
    "  pipe = Pipeline([(\"scaler\", StandardScaler()), #first: name of pipeline, second: Transformer or Predictor\n",
    "                  (\"LogReg\", LogisticRegression(penalty='l1', C=term, max_iter=3000, solver='liblinear'))])\n",
    "\n",
    "  scores = cross_val_score(pipe,\n",
    "                          X_train,\n",
    "                          y_train,\n",
    "                          cv=5, # number of folds\n",
    "                          scoring='roc_auc')\n",
    "\n",
    "  cv_R2 = np.mean(scores)\n",
    "  if cv_R2 > max_R2:\n",
    "    max_R2 = cv_R2\n",
    "    best_expo = term\n",
    "  print(f\"mean CV R^2 ({term=}):\", np.round(cv_R2, 4))\n",
    "  print(f\"std CV R^2 ({term=}):\", np.round(np.std(scores), 4), '\\n')\n",
    "\n",
    "print(\"Best CV R^2:\", max_R2)\n",
    "print(\"Best expo:\", best_expo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean CV R^2 (term=0.001): 0.982\n",
      "std CV R^2 (term=0.001): 0.0076 \n",
      "\n",
      "mean CV R^2 (term=0.01): 0.9813\n",
      "std CV R^2 (term=0.01): 0.0073 \n",
      "\n",
      "mean CV R^2 (term=0.1): 0.9775\n",
      "std CV R^2 (term=0.1): 0.0074 \n",
      "\n",
      "mean CV R^2 (term=1): 0.9763\n",
      "std CV R^2 (term=1): 0.007 \n",
      "\n",
      "mean CV R^2 (term=10): 0.9761\n",
      "std CV R^2 (term=10): 0.0069 \n",
      "\n",
      "mean CV R^2 (term=100): 0.9761\n",
      "std CV R^2 (term=100): 0.0069 \n",
      "\n",
      "mean CV R^2 (term=1000): 0.9761\n",
      "std CV R^2 (term=1000): 0.0069 \n",
      "\n",
      "Best CV R^2: 0.9819651806297547\n",
      "Best expo: 0.001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "max_R2 = -np.inf\n",
    "best_expo = None\n",
    "reg_terms = [0.1, 1, 10]\n",
    "reg_terms2 = [10**i for i in [-3, -2, -1, 0, 1, 2, 3]] #[0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for term in reg_terms2:\n",
    "  pipe = Pipeline([(\"scaler\", StandardScaler()), #first: name of pipeline, second: Transformer or Predictor\n",
    "                  (\"LogReg\", LogisticRegression(penalty='l2', C=term, max_iter=3000, solver='lbfgs'))]) #solver='saga'\n",
    "\n",
    "  scores = cross_val_score(pipe,\n",
    "                          X_train,\n",
    "                          y_train,\n",
    "                          cv=5, # number of folds\n",
    "                          scoring='roc_auc')\n",
    "  #print(scores)\n",
    "  cv_R2 = np.mean(scores)\n",
    "  if cv_R2 > max_R2:\n",
    "    max_R2 = cv_R2\n",
    "    best_expo = term\n",
    "  print(f\"mean CV R^2 ({term=}):\", np.round(cv_R2, 4))\n",
    "  print(f\"std CV R^2 ({term=}):\", np.round(np.std(scores), 4), '\\n')\n",
    "\n",
    "print(\"Best CV R^2:\", max_R2)\n",
    "print(\"Best expo:\", best_expo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "max_R2 = -np.inf\n",
    "best_expo = None\n",
    "reg_terms = [0.1, 1, 10]\n",
    "reg_terms2 = [10**i for i in [-3, -2, -1, 0, 1, 2, 3]] #[0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "for term in reg_terms:\n",
    "  pipe = Pipeline([(\"scaler\", StandardScaler()), #first: name of pipeline, second: Transformer or Predictor\n",
    "                  (\"LogReg\", LogisticRegression(penalty='elasticnet', C=10**term, max_iter=3000, solver='saga'))]) #solver='saga'\n",
    "\n",
    "  scores = cross_val_score(pipe,\n",
    "                          X_train,\n",
    "                          y_train,\n",
    "                          cv=5, # number of folds\n",
    "                          scoring='roc_auc')\n",
    "\n",
    "  cv_R2 = np.mean(scores)\n",
    "  if cv_R2 > max_R2:\n",
    "    max_R2 = cv_R2\n",
    "    best_expo = term\n",
    "  print(f\"mean CV R^2 ({term=}):\", np.round(cv_R2, 4))\n",
    "  print(f\"std CV R^2 ({term=}):\", np.round(np.std(scores), 4), '\\n')\n",
    "\n",
    "print(\"Best CV R^2:\", max_R2)\n",
    "print(\"Best expo:\", best_expo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "#                  (\"LogReg\", LogisticRegression())])\n",
    "\n",
    "# C = [10**expo for expo in [ -2, -1, 0, 1, 2]] #[0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# # The following builds a dictionary of lists of values for the hyper-parameters\n",
    "# # NOTE: The name of the parameter `alpha` of the pipeline step called `ridge`\n",
    "# #       is `ridge__alpha`.\n",
    "# param_grid = {\"LogReg Cs\" : C} \n",
    "\n",
    "# grid = GridSearchCV(pipe, # model for which good hyperparameters should be found\n",
    "#                     param_grid=param_grid, # dictionary determining the parameters to search\n",
    "#                     cv=5) # determine the value of k for k-fold CV (here, k=5)\n",
    "\n",
    "# # Calling `fit` performs a search for the best hyper-parameter values using k-fold CV\n",
    "# # Furthermore, it fits a model on the FULL TRAINING DATA,\n",
    "# # using the best found choice for the hyper-parameters\n",
    "# grid.fit(X_res, y_res)\n",
    "\n",
    "\n",
    "# # We can access the best parameter choice and the corresponding CV score as follows\n",
    "# print(\"Best CV score:\", grid.best_score_)\n",
    "# print(\"Best parameter:\", grid.best_params_)\n",
    "\n",
    "# # If we use `grid.score` or `grid.predict`, this uses the model trained on the\n",
    "# # full training data, with the best hyper-parameter found using k-fold CV\n",
    "# print(\"Test set score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AUC-Score with tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on train_set imbalanced dataset: 0.768671825644262\n",
      "AUC on test_set imbalanced dataset: 0.7799648283624087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Without SMOTE\n",
    "logreg = LogisticRegression(penalty='l2', C=0.001, max_iter=3000, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print('AUC on train_set imbalanced dataset: {}'.format(roc_auc_score(y_train, logreg.predict(X_train)))) #0.9819\n",
    "print('AUC on test_set imbalanced dataset: {}\\n'.format(roc_auc_score(y_test, logreg.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on train_set smoted dataset: 0.9539465159071303\n",
      "AUC on test_set smoted dataset: 0.9530008441193021\n"
     ]
    }
   ],
   "source": [
    "# With SMOTE\n",
    "logreg = LogisticRegression(penalty='l2', C=0.001, max_iter=3000, solver='lbfgs')\n",
    "logreg.fit(X_res, y_res)\n",
    "\n",
    "print('AUC on train_set smoted dataset: {}'.format(roc_auc_score(y_res, logreg.predict(X_res))))\n",
    "print('AUC on test_set smoted dataset: {}'.format(roc_auc_score(y_test, logreg.predict(X_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
