---
title: "Data Analytics Challenge - ASN-SMOTE"
output:
  pdf_document: default
  html_document: default
date: "2023-07-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = FALSE
)
```

<style>
p {
  text-align: justify;
}
</style>


Philipp von Lovenberg, Vincent Bläske, Dennis Götz

## Inhaltsverzeichnis

1.	Einleitung  
2.	Data Exploration  
3.	Auswahl und Implementierung von SMOTE und Classifier  
    3.1.  Anwendung des SMOTE Algorithmus  
    3.2.  ASN-SMOTE  
    3.3.  Verwendete Classifier  
    3.4.  Performancemaß
4.	Programmablauf mit Cross-Validation und Hyperparameteroptimierung  
    4.1.	Undersampling  
    4.2.	Cross-Validation  
    4.3.	Hyperparameteroptimierung  
    4.4.  Skript  
    4.5.	Ergebnisse der Performance  
5.	Ausblick  


## 1. Einleitung
Derzeit stellen unausgeglichene Datensätze eine erhebliche Herausforderung im Bereich des maschinellen Lernens und des Data Minings dar. Denn die herkömmlichen Klassifizierungsverfahren neigen in der Regel dazu, die Mehrheitsklasse zu bevorzugen, welche bei einem binären Klassifizierungsproblem oft ein Vielfaches an Instanzen gegenüber der Minderheitsklasse besitzt. Hierdurch wird der Klassifikator in seiner Vorhersagefähigkeit, die Minderheitsinstanzen korrekt zu erkennen, stark beeinträchtigt, kann jedoch trotzdem eine hohe Accuracy aufweisen. Die Minderheitsklasse kann dabei auch völlig unerkannt bleiben. Diese Thematik ist insbesondere bei der Erkennung betrügerischer Transaktionen in Banken, Kreditrisikobewertung oder Erkennung von Firewall-Eingriffen von hoher Bedeutung. Aufgrund dieser universellen Existenz von unausgewogenen Datensätzen wurden bereits viele Vorverarbeitungsmethoden vorgeschlagen, um die Imbalance zu bewältigen. Oversampling ist eine vielversprechende Technik für unausgewogene Datensätze, die neue Minderheiteninstanzen erzeugt, um den Datensatz auszugleichen. Unter den Oversampling-Methoden ist die Synthetic Minority-Oversampling-Technique (SMOTE) eine der bekanntesten Methoden, die künstliche Minoritätsinstanzen durch lineare Interpolation erzeugt. Eine jüngst veröffentlichte Adaption dessen liefert im Paper „ASN-SMOTE: a synthetic minority oversampling method with adaptive qualified synthesizer selection“ von Yi Xinkai et al. (2022) vielversprechende Ergebnisse, weshalb dieser Algorithmus im Folgenden auf dem Creditcard Datensatz angewendet wird. Weiterhin wird seine Performance mithilfe zweier geeigneter Classifier und den Ergebnissen des originalen SMOTE Algorithmus untersucht. Dafür wird nach der Data Exploration und Anwendung des SMOTE im 1. Kapitel die Funktionsweise des ASN-SMOTE sowie die Unterschiede zum ursprünglichen Algorithmus in Kapitel 2 erläutert. Anschließend werden die verwendeten Classifier und das Performancemaß in Kapitel 3 sowie die gewählte Cross-Validation im 4. Kapitel präsentiert. Darauf folgt in Kapitel 5 und 6 eine Methode des Undersamplings und die Durchführung der Hyperparameteroptimierung. Abschließend werden die Ergebnisse des ASN-SMOTE in Kapitel 6 analysiert, bevor ein Ausblick mit weiteren Ideen und Forschungsfragen folgt. Der Code des entsprechenden Abschnitts ist zur Nachvollziehbarkeit in der Programmiersprache R an jedes Kapitel angehängt.


## 2. Data Exploration
Der zu behandelnde Datensatz Creditdata beinhaltet europäische Kreditkartentransaktionen von September 2013 im CSV-Format und besitzt pro Transaktion 31 Features/Spalten, wobei die letzte binäre Spalte die Target darstellt. Diese definiert, ob ein Sample Fraud (1) oder kein Fraud (0) ist. Das Ziel hierbei ist es ein Machine Learning Modell zu trainieren, welcher Kreditkartenbetrug erkennen soll, um die Kunden vor unautorisierten Belastungen zu schützen. Die Daten sind ausschließlich numerisch und abgesehen von Time und Amount aus Datenschutzgründen mit PCA transformiert worden. Zu Beginn werden die notwendigen Packages geladen sowie die Working Directory in den Ordner gesetzt, worin sich der Creditdata Datensatz befindet und mit read.csv eingelesen. Anschließend wird eine NA-Analyse durchgeführt und einige Statistiken der einzelnen Spalten berechnet. Die Imbalance des Datensatzes wird deutlich, wenn wir die Anzahl der Minority Samples mit den 284.315 der Majority vergleichen. Denn es befinden sich lediglich 492 Fraud Samples in Creditdata, was einem Prozentsatz von 0,17% entspricht. Dies deutet auf eine extreme Imbalance zwischen Majority und Minority hin und macht eine Preprocessingmethode umso notwendiger. Aufgrund der hohen Imbalance wird ein 90 zu 10 Split der Daten in Trainings- und Testdatensatz durchgeführt, worauf eine Trennung der Features von der Target Variable folgt. Da das Feature Time als nicht aussagekräftig genug erscheint, wird dieses aus dem Datensatz entfernt und ebenfalls ein Scaling der Spalte Amount vorgenommen, da diese nicht mit PCA transformiert wurde. Dies geschieht in Trainings- und Testdatensatz separat, um den Mittelwert und die Standardabweichung des Testdatensatzes nicht mit in den Trainingsdatensatz aufzunehmen, was andernfalls zu Overfitting führen würde. Beim Korrelationsplot der Features mit der Target Variable fallen einige positive als auch negative Korrelation beim Feature Amount beispielsweise mit V2 und V7 auf. Die Target Value weist z.B. mit den Features V12, V15 und V17 lediglich einige niedrige negative Korrelation auf. Dies liegt insbesondere an der PCA Transformation, welche in Voraus durchgeführt wurde. Aus diesem Grund werden keine Features selektiert.

```{r Load packages, message=FALSE, warning=FALSE}

# Load necessary packages
library(tidyverse)
library(randomForest)
library(caTools)
library(smotefamily)
library(caret)
library(mlr)
library(tibble)
library(xgboost)
library(pROC)
library(ggplot2)
library(parallelMap)
library(parallel)
library(glmnet)
library(tidymodels)
library(corrplot)
```

```{r Data Exploration}
# Load the dataset

ccdata <- read.csv("~/creditcard.csv")

# look at the data
#View(ccdata)
#summary(ccdata)         # summary statistics of the data
colSums(is.na(ccdata))  # check for NA in the data
table(ccdata$Class)     # absolute amount of class membership


### Preprocessing for usage without cross validation
# Split into training/test set
set.seed(123)
split <- sample.split(ccdata$Class, SplitRatio = 0.9)
train <- subset(ccdata, split == TRUE)
test <- subset(ccdata, split == FALSE)
table(train$Class)

# Drop column 'Time' and scale column 'Amount' individually for train and test set
train <- train[,-1] %>% mutate(Amount = scale(Amount))
test <- test[,-1] %>% mutate(Amount = scale(Amount))

# Split features from the target value
y_train <- train$Class
X_train <- train[,-30]

### some data exploration (plot of time and amount for the 2 classes)
# correlation
corr_plot <- corrplot(cor(train), method = "circle", type = "upper")
```

## 3. Auswahl und Implementierung von SMOTE und Classifier  
### 3.1. Anwendung des SMOTE Algorithmus  

Im Anschluss an die Data Exploration wird der in R bereits implementierte SMOTE Algorithmus aus dem Package smotefamily auf den Trainingsdatensatz angewendet, um eine Balance im Datensatz herzustellen. Nach der Generierung von 255.228 synthetischen Trainingsdaten beträgt der Anteil der Minority Samples im Train Set 49,97%. Beim Vergleich der Visualisierungen mit den Features V1 und V2 wird die Funktionsweise des SMOTE Algorithmus mit linearer Interpolation durch die geraden Linien, welche die synthetischen Daten ziehen, deutlich.

```{r Anwendung des SMOTE Algorithmus}
### Apply the original SMOTE Algorithmus to the train set
# Calculate number of synthetic samples for each minority instance
n_smote <- as.integer((255884 - 443)/443)

set.seed(1234)
smote_ <- smotefamily::SMOTE(X = X_train, target= y_train, K= 5, dup_size=n_smote)
train_smote <- (smote_$data)
train_smote$class <- as.numeric(train_smote$class)
colnames(train_smote)[colnames(train_smote) == "class"] <- "Class"
# View the new balance in the dataset
table(train_smote$Class)
prop.table(table(train_smote$Class))

# data visualization
# Plot the first two features with the target value before SMOTE
ggplot(train, aes(x = V1, y = V2)) +
  geom_point(data = train[train$Class == 0,], alpha = 0.3, color = "#E69F00") +  
  geom_point(data = train[train$Class == 1,], alpha = 1, color = "#56B4E9") +
  ggtitle("Class distribution before SMOTE") +
  xlim(-30, 3) + 
  ylim(-20, 20) 
# Plot the first two features with the target value after SMOTE
ggplot(train_smote, aes(x = V1, y = V2)) +
  geom_point(data = train_smote[train_smote$Class == 0,], alpha = 0.3, color = "#E69F00") +  
  geom_point(data = train_smote[train_smote$Class == 1,], alpha = 0.3, color = "#56B4E9") +
  ggtitle("Class distribution after SMOTE (1:1)") +
  xlim(-30, 3) + 
  ylim(-20, 20) 
```


### 3.2 ASN-SMOTE
Allerdings können unsachgemäß erzeugte Minority Samples das Lernen des Classifiers auch beeinträchtigen und sich negativ auf ihn auswirken. Da im originalen SMOTE Algorithmus sowohl alle Minority Instanzen als auch alle k-nearest neighbors gleich behandelt werden und somit zur Generierung synthetischer Datenpunkte verwendet werden, kann dies zu einem Verschwimmen der Entscheidungsgrenze bzw. Überlappen der Feature Spaces führen. Durch die synthetischen Daten im Feature Space der Majority Class, wird es für ein Modell schwierig die beiden Klassen zu unterscheiden. Auf dieses Problem zielt der einfache und effektive Ansatz des ASN-SMOTE ab, welcher ebenfalls auf der k-Nearest-Neighbors und SMOTE-Technologie basiert. Dieser lässt sich in drei Algorithmen unterteilen, wobei der erste als Noise-Filtering bezeichnet wird. Hierbei wird für jede Minority Instanz die euklidische Distanz zu allen anderen Datenpunkten berechnet und lediglich der nächste Nachbar betrachtet. Sollte dieser zur Majority gehören, wird die gerade betrachtete Minority Instanz als Noise klassifiziert und im weiteren Verlauf nicht als Synthesizer für die Datengenerierung verwendet. Sollte der nächste Nachbar ebenfalls zur Minority gehören, ist die betrachtete Minority Instanz qualifiziert für die Generierung synthetischer Daten. Dies dient zur Filterung von Noise und Instanzen entlang der Entscheidungsgrenze. Im Anschluss folgt der Algorithmus zur adaptiven Nachbarauswahl, wobei für die k nächsten Nachbarn erneut die euklidische Distanz berechnet wird. Befindet sich darunter eine Majority Instanz, werden nur diejenigen als Nachbar für die Synthese genutzt, welche eine geringere Distanz als die nächste Majority Instanz aufweisen. Alle anderen Nachbarn werden als unqualifiziert markiert und nicht für die Datengenerierung mit der betrachteten Minority Instanz verwendet. Der dritte Algorithmus dient zur Erstellung der Samples, wobei anfangs die Anzahl der notwendigen neuen Instanzen für jedes Minority Sample bis zur optimalen Balance berechnet werden. Daraufhin folgt die Anwendung des SMOTE Algorithmus, welcher die synthetischen Daten mithilfe der linearen Interpolation erstellt. Dabei wird die euklidische Distanz zwischen der Synthesizer Minority Instanz und einem zufällig ausgewählten Nachbar der k nächsten Nachbarn (bei ASN: nur qualifizierten k nächsten Nachbarn) mit einer Zufallszahl zwischen Null und Eins multipliziert und zur Synthesizer Instanz addiert. Anschließend wird zu jeder erstellten Instanz die Target Variable (1) hinzugefügt und die synthetischen Daten an den Trainingsdatensatz angehängt. Die Vorteile des neuen Ansatzes sind dabei die Effektivitätssteigerung durch das Filtern von Noise sowie die Feature Spaces daran zu hindern sich zu überlappen. Die Wirksamkeit dieses modernen ASN-SMOTE Algorithmus wurde mit 24 unausgewogenen Datensätzen im Originalpaper getestet und zeigte durchweg positive Ergebnisse im Vergleich mit anderen namhaften SMOTE Adaptionen. Die Imbalance der 24 Datensätze lag dabei zwischen 1,82% und 41,4%, weshalb die folgenden Fragen aufkommen: Funktioniert der ASN-SMOTE auch beim Creditdata Datensatz, welcher eine deutlich höhere Imbalance von 0,17% aufweist? Oder ist möglicherweise ein vorausgezogenes Undersampling notwendig? Bei der Anwendung des Algorithmus auf den gegebenen Datensatz, zeigt sich, dass 82 der lediglich 442 Minority Samples im Trainingsdatensatz als unqualifiziert klassifiziert werden, wodurch dem Classifier das Lernen noch mehr erschwert wird. Bei der Analyse von höheren Werten für den Hyperparameter k wird deutlich, dass nur die wenigsten Minority Instanzen 30 nächste Nachbarn aus der eigenen Klasse oder mehr besitzen und keine mehr als 72. Daher wird beim Tuning ein höheres Augenmerk auf niedrige Wert für k gelegt und dieser Bereich feiner unterteilt als hohe Werte für k. Aufgrund von Unstimmigkeiten im ASN-Paper wurde „n“ die Anzahl der zu generierenden Samples pro qualifizierter Minority Instanz sowohl als Inputparameter festgelegt als auch in der Funktion selbst „n_opt“ berechnet. Durch Auskommentieren der Codezeile „n <- n_opt“ kann n als Eingabewert verwendet werden und andernfalls wird jenes n gewählt, welches zur optimalen Balance im Datensatz führt. Beim Plot der Features V1 und V2 wird fällt anhand der zwei gekennzeichneten Punkte der Unterschied zum SMOTE Ansatz deutlich auf, da die vom ersten Algorithmus als Noise klassifizierte blaue Minority Instanz nicht für die Generierung synthetischer Daten verwendet wird. Zudem wählt der Algorithmus zur Nachbarauswahl den zweiten gekennzeichneten Punkt der Majority Class nicht als qualifizierten Nachbar aus. Das Verhalten des ASN ist bei Zweiterem aber möglicherweise ungünstig, da es sich hierbei um einen Ausreißer der Majority handeln könnte, welcher im Feature Space der Minority liegt.

```{r ASN-SMOTE}

# ASN-SMOTE function
asn_smote <- function(train, n, k) {

  # Split the train set into features (X_train) (T in the Pseudo Code) and target value (y_train)

  
  # Create a matrix with the features and split the train set into majority and minority
  train_feat_matrix <- as.matrix(X_train)
  train_Majority <- train[y_train == 0,]
  train_Minority <- train[y_train == 1,]
  
  # Select only the features of the minority train set (P in the Pseudo code)
  train_Minority_feat <- train_Minority[,1:29]

  # Calculate the distance of each minority instance to all samples of the train set
  dis_matrix <- proxy::dist(train_Minority_feat, X_train)
  
  # Create a list with indices of the k-nearest minority neighbors of all minority
  # instances (majority neighbors marked as NaN)
  index_knn <- list()
  
  for (i in 1:nrow(train_Minority_feat)) {
    index_knn[[rownames(dis_matrix)[i]]] <- order(dis_matrix[i,])[2:(k+1)]
    for (j in 1:k) {
      if (y_train[index_knn[[i]][j]] == 0 ) {
        index_knn[[i]][j] <- NaN
      }
    }
  }
  
  print("Distance matrix calculated and nearest neighbors defined.")
  print("--------------------------------------------------------------------------------")
  
  
  
  # Algorithm 1: Filter Noise
  # Drop minority instances with a majority (NaN) as nearest neighbor
  Mu <<- vector()
  
  for (i in length(index_knn):1) { 
    if (is.nan(index_knn[[i]][1])) {
      Mu[i] <<- names(index_knn[i])
      index_knn <- index_knn[-i]
    }
  }
  
  print(paste0("Number of qulaified minority instances: ", length(index_knn), 
               " of ", nrow(train_Minority)))
  print("Algorithm 1 successfully completed.")
  print("--------------------------------------------------------------------------------")
  
  
  # Algorithm 2: Adaptive neighbor instances selection
  # Keep only the neighbors that are closer than the nearest majority (NaN) instance
  for (i in 1:length(index_knn)) {
    for (j in 1:k) {
      if (is.nan(index_knn[[i]][j])) {
        index_knn[[i]] <- index_knn[[i]][1:(j-1)]
        break
      }
    }
  }

  print(paste0("Mean qualified nearest neighbors: ", 
               round(sum(lengths(index_knn))/length(index_knn), 2), " of ", k))
  print(paste0("Maximum qualified nearest neighbors: ", 
               max(sapply(index_knn, length)), " of ", k))
  hist_qn <<- hist(sapply(index_knn, length), plot=FALSE)
  print("Algorithm 2 successfully completed.")
  print("--------------------------------------------------------------------------------")

  
  # Algorithm 3: Procedure of ASN-SMOTE (Create new synthetic minority samples)
  # Add to the feature values of each qualified minority instance the difference of the 
  # minority sample and one random selected neighbor of their qualified neighbors
  # multiplied with a random number between 0 and 1 for n times.
  
  # Calculate the amount of synthetic minority samples for each qualified minority sample
  # that the train set is balanced
  n_opt <- as.integer((nrow(train_Majority) - nrow(train_Minority))/length(index_knn))
  print(paste0("optimal n = ", n_opt))
  
  #if you assign 'n_opt' to 'n', then 'n' is not a inputparameter anymore
  # always the perfect balance will be generated in the dataset
  #n <- n_opt
  
  synthetic <- list()
  for(i in names(index_knn)) {
    for(j in seq_len(n)) {
      random_n <- sample(seq_along(index_knn[[i]]), 1)
      dif <- train_feat_matrix[index_knn[[i]][random_n],] - train_feat_matrix[i,]
      randomNum <- runif(1)
      synthetic_instance <- train_feat_matrix[i,] + randomNum * dif
      synthetic[[length(synthetic) + 1]] <- synthetic_instance
    }
  }
  
  print(paste0("Number of generated synthetic minority samples: ", length(synthetic)))
  print("Algorithm 3 successfully completed.")
  print("--------------------------------------------------------------------------------")
  
  
  
  # Assign "Class" label = 1 to the synthtic points
  synthetic_df <- do.call(rbind, synthetic)
  synthetic_df <- as.data.frame(synthetic_df)
  synthetic_labels <- rep(1, length(synthetic))
  synthetic_df$Class <- synthetic_labels
  
  # Combine original train set with synthetic set
  asn_train <<- rbind(train, synthetic_df)
  print("The ASN-SMOTE was applied to the data.")
  print("The new training dataset is saved as 'asn_train'.")
  print("--------------------------------------------------------------------------------")
  
  # View the new balance of the dataset
  print("New balance of 'creditcard' dataset:")
  return (table(asn_train$Class))
}
```

```{r Execute ASN-SMOTE function}
# Execute ASN-SMOTE function (optimal balanced with n=707)
train_asn_smote <- asn_smote(train, n=707, k=5)

# Execute ASN-SMOTE function with high 'k' (very high k doesn't make sense!)
asn_smote(train, n=10, k=100)
plot(hist_qn, xlab = "Neighbors", ylab = "Frequency", xlim = c(0, 100), ylim = c(0, 200),
     col = "lightblue", main = "Histogram of qualified nearest neighbors")


# data visualization after ASN-SMOTE
ggplot(train_smote, aes(x = V1, y = V2)) +
  geom_point(data = asn_train[asn_train$Class == 0,], aes(color = "Majority Class"), alpha = 0.3) + 
  geom_point(data = asn_train[asn_train$Class == 1,], aes(color = "Minority Class"), alpha = 1) +
  geom_point(data = train[Mu, ], aes(color = "Unqualified Minority Class"), alpha = 1, size = 3, shape = 21) +
  ggtitle("Class distribution with unqualified data after ASN SMOTE") +
  xlim(-30, 3) + 
  ylim(-20, 20)+
  scale_color_manual(values = c("#E69F00", "#56B4E9", "red"),labels = c("Majority Class", "Minority Class", "Unqualified Minority Class")) +
  guides(color = guide_legend())
```



### 3.3 Verwendete Classifier
Um für das Testdatenset Vorhersagen treffen zu können, müssen wir erst ein Modell aufstellen und dieses tunen. Hierbei entscheiden wir uns für die Machine Learning Modelle der Logistischen Regression und des Random Forest. Die Logistische Regression eignet sich beispielsweise gut für die Schätzung der Wahrscheinlichkeiten von binären oder kategorialen abhängigen Variablen. Der grundlegende Ansatz der logistischen Regression besteht darin, dass eine Sigmoid-Funktion auf eine lineare Kombination der Features angewendet wird. Die Sigmoid-Funktion transformiert den linearen Ausdruck in einen Wert zwischen 0 und 1, der als Wahrscheinlichkeit interpretiert werden kann. Zudem findet eine Regularisierung statt, welche mit dem Hyperparameter C festgelegt wird. Dieser ist auch der einzige zu tunende Parameter im Modell, wodurch eine Cross-Validation mit geringerem Rechenleistung durchgeführt werden kann. Jedoch könnte die vom Modell angenommene lineare Beziehung zwischen den Features einen Nachteil darstellen. Im Bezug auf den Random Forest ist zu wissen, dass er zu den Ensemble-Lernverfahren gehört und für Klassifikations- und Regressionsaufgaben verwendet wird. Er besteht aus einer Kombination von Entscheidungsbäumen, die unabhängig voneinander trainiert werden. Dabei wird jeder Baum auf einer zufälligen Stichprobe der Trainingsdaten trainiert und betrachtet auch nur eine zufällige Teilmenge der verfügbaren Features. Dies hilft dem Modell, um Overfitting zu vermeiden und gut verallgemeinern zu können. Bei einer Klassifikationsaufgabe ist die Vorhersage üblicherweise eine Mehrheitsentscheidung der Bäume. Die Vorteile dieser Methode sind, dass der Random Forest problemlos mit großen Datenmengen arbeiten kann und nicht dazu neigt die Trainingsdaten auswendig zu lernen. Zudem schneidet er in der Praxis oft besser ab als andere Classifier. Beim Tuning sind jedoch deutlich mehr Hyperparameter zu optimieren, wodurch ein großer Rechenaufwand notwendig ist und das Modell ist meist schwieriger zu interpretieren als einfachere Verfahren, wie z.B. die Logistische Regression.


```{r Logistische Regression}

log_regression <- function(train) {
    train$Class <- as.factor(train$Class)
    
    # Define the logistic regression model
    log_reg <- logistic_reg(mode = "classification", engine = "glmnet", penalty = tune(), mixture = tune())
    
    # Define the grid search for the hyperparameters
      # tries out 3 variations of the penalty hyperparameter and 4 varaitions of the mixture hyperparameter
    grid <- grid_regular(penalty(), mixture(), levels = c(penalty = 3, mixture = 4)) 
    
    # Define the workflow for the model
    log_reg_wf <- workflow() %>%
      add_model(log_reg) %>%
      add_formula(Class ~ .)
    
    # Define the resampling method for the grid search
    folds <- vfold_cv(data = train, v = 5)                   
    
    # Tune the hyperparameters using the grid search
    log_reg_tuned <- tune_grid(
      log_reg_wf,
      resamples = folds,
      grid = grid,
      control = control_grid(save_pred = TRUE)
    )
    
    # Select the best model based on the metric
    best_model <- select_best(log_reg_tuned, metric = "roc_auc")
    
    # Extract the best hyperparameters
    best_penalty <- best_model$penalty
    best_mixture <- best_model$mixture
    
    # Fit the model using the optimal hyperparameters
    log_reg_final <- logistic_reg(penalty = best_penalty, mixture = best_mixture) %>%
      set_engine("glmnet") %>%
      set_mode("classification") %>%
      fit(Class~., data = training)
    
    # Evaluate the model performance on the test set
    pred_class <- predict(log_reg_final,
                          new_data = test,
                          type = "class")
    
    return(pred_class$.pred_class)
    
}

```


```{r XGBoost}

xgboost <- function(train) {
  
    train$Class <- as.factor(train$Class)
    # Define the task
    classif_task <- makeClassifTask(id = "credit", data = train, target = "Class")     
    
    # Set the learner
    learner <- makeLearner("classif.xgboost", predict.type = "prob", fix.factors.prediction = TRUE)
    
    # Define the parameter set
    params <- makeParamSet(
          makeIntegerParam("nrounds", lower = 100, upper = 500),
          makeNumericParam("eta", lower = 0.05, upper = 0.3)
          )
    
    # Set the control for tuning
    ctrl <- makeTuneControlGrid()
    
    # Set resampling strategy
    rdesc <- makeResampleDesc("CV", iters = 5L)
    
    # Tune the hyperparameters
    tune_result <- tuneParams(learner, task = classif_task, resampling = rdesc, par.set = params, control = ctrl)
    
    # Set the tuned parameters
    learner_tuned <- setHyperPars(learner, par.vals = tune_result$x)
    
    # Train the model
    final_model <- mlr::train(learner_tuned, classif_task)
    
    # Make predictions on the test set
    test$Class <- as.factor(test$Class)
    test_pred <- predict(final_model, newdata = test, type = "prob")
    
    return(test_pred$data$response)
}

```

```{r Train the Classifiers on SMOTE and on the ASN Smote}

pred_log_reg_smote <- log_regression(train_smote)
pred_xgboost_smote <- xgboost(train_smote)

pred_log_reg_asn_smote <- log_regression(train_asn_smote)
pred_xgboost_asn_smote <- xgboost(train_asn_smote)

```



### 3.4 Performancemaß
Die Leistung der Algorithmen des maschinellen Lernens wird in der Regel anhand einer Konfusionsmatrix bewertet und die Vorhersagegenauigkeit definiert als Accuracy = (T P + T N)/(T P + F P + T N + F N). Dabei ist die Fahlerrate 1 – Accuracy. Dies gilt jedoch nur im Zusammenhang mit ausgeglichenen Datensätzen und gleichen Fehlerkosten. Denn beim betrachteten Datensatz würde sogar ein Modell, welches immer die Majority prognostiziert, eine Accuracy von über 99% erreichen. Daher verwenden wir beim Creditdata Datensatz mit ungleichen Fehlerkosten, die Receiver Operating Characteristic Curve (ROC-Curve) und die Area under the Curve (AUC), um die Performance der Logistischen Regression und des Random Forests zu bestimmen. Hierbei wird auf der X-Achse die False-Positive-Rate %F P = F P/(T N+F P) und auf der Y-Achse die True-Positive-Rate %T P = T P/(T P +F N) dargestellt. Der ideale Punkt auf der ROC-Kurve wäre (0,100), d. h. alle positiven Samples werden richtig klassifiziert und keine negativen Beispiele werden falsch als positiv klassifiziert. Die Linie y = x stellt das Szenario einer zufälligen Schätzung der Klasse dar. Zudem ist die Fläche unter der ROC Kurve (AUC) ein nützliches Maß für die Leistung des Klassifikators, da es unabhängig vom gewählten Entscheidungskriterium und den vorherigen Wahrscheinlichkeiten ist.

```{r Performance Function}
performance <- function(title, pred) {
  
  pred <- as.factor(pred)
  test$Class <- as.factor(test$Class)
  auc <- roc(test$Class, as.numeric(pred))

  truth_predicted <- data.frame(
    obs = test$Class,
    pred = pred
  )
  
  levels(truth_predicted$obs) <- c("not_Fraud", "Fraud")
  levels(truth_predicted$pred) <- c("not_Fraud", "Fraud")
  
  cm <- conf_mat(truth_predicted, obs, pred)
  
  TP <- cm$table[2, 2]
  FN <- cm$table[1, 2]
  FP <- cm$table[2, 1]
  TN <- cm$table[1, 1]
  
  # Sensitivity (true positive rate)
  sensitivity <- TP / (TP + FN)
  
  # Specificity (true negative rate)
  specificity <- TN / (TN + FP)
  
  cm_plot <- autoplot(cm, type = "heatmap") +
                      scale_fill_gradient(low = "#D6EAF8", high = "#2E86C1") +
                      labs(title = paste0(title, " Performance"),
                      subtitle = paste0("AUC: ",round(pROC::auc(auc), 4), "\n","\n",
                                        "Sensitivity: ", round(sensitivity,4),  
                                        "  Specificity: ", round(specificity,4))) 

  
  
  return(list(AUC = auc_plot, Confusion_Matrix = cm_plot, Sensitivity = sensitivity, Specificity = specificity))
}

```


## Performance Logistic Regression mit SMOTE
```{r}
performance("Logistische Regression mit SMOTE", pred_log_reg_smote)
```
## Peformance XGBoost mit SMOTE
```{r}
performance("XGBoost mit SMOTE", pred_xgboost_smote)
```

## Peformance Logistische Regression mit ASN SMOTE
```{r}
performance("Logistische Regression mit ASN SMOTE", pred_log_reg_asn_smote)
```

## Peformance XGBoost mit ASN SMOTE
```{r}
performance("XGBoost mit ASN SMOTE", pred_xgboost_asn_smote)
```


```{r Wo wurden die Fehler gemacht?}
ggplot(train_smote, aes(x = V1, y = V2)) +
  geom_point(data = train_smote[train_smote$Class == 0,], alpha = 0.4, color = "#E69F00") +  
  geom_point(data = train_smote[train_smote$Class == 1,], alpha = 0.4, color = "#56B4E9") +
  geom_point(data = test[pred_xgboost_smote == 0 & test$Class == 1, ], alpha = 1, color = "red", size = 3) +
  ggtitle("FALSE NEGATIVE bei XGBoost mit SMOTE") +
  xlim(-6, 3) + 
  ylim(-3, 5) 

ggplot(train_asn_smote, aes(x = V1, y = V2)) +
  geom_point(data = train_asn_smote[train_asn_smote$Class == 0,], alpha = 0.4, color = "#E69F00") +  
  geom_point(data = train_asn_smote[train_asn_smote$Class == 1,], alpha = 0.4, color = "#56B4E9") +
  geom_point(data = test[pred_xgboost_asn_smote == 0 & test$Class == 1, ], alpha = 1, color = "red", size = 3) +
  ggtitle("FALSE NEGATIVE bei XGBoost mit ASN SMOTE") +
  xlim(-6, 3) + 
  ylim(-3, 5) 


```

## Interpretation der Ergebnisse
Wenn man sich jetzt genauer anschaut wo die Fehler beim Smote und wo die Fehler (False Negative --> als nicht_fraud prognostiziert aber in Wahrheit Fraud) beim ASN Smote lagen (hier nur 2 dimensional), sieht man dass die Fehler in den selben Bereichen liegen. Der Unterschied liegt darin, dass bei der Anwendung des ASN Smotes, etwas mehr False negative entstanden sind (auch an der confusion matrix zu erkennen) 



## 4. Programmablauf mit Cross-Validation und Hyperparameteroptimierung
### 4.1 Undersampling

Der vorliegende Datensatz ist mit seinem Verhältnis zwischen Majority Class und Minority Class von ca. 578:1 deutlich stärker imbalanced als die Datensätze, mit denen ASN-SMOTE im dazugehörigen Paper getestet wurde (1,82 - 41,4). Deshalb und aufgrund der herausfordernden langen Laufzeiten wurde sich dafür entschieden, die majority class zu undersamplen. Dafür wurde eine Funktion für random undersampling implementiert. 


```{r Undersampling}

# Input: undersamplingFaktor (0 = kein Undersampling, 0.99 = Um 99 %, also 1% der urspr. Anzahl), Trainingsdatensatz
undersample <- function (undersamplingFactor = 0.99, dataset = ccdata){
  # TRUE im SelectionVector heißt weiter benutzen, FALSE heißt verwerfen
  selectionVector <- dataset$Class==1
  selectionVector <- as.logical(
    selectionVector
    +
      sample(c(0,1),length(dataset[,1]), prob = c(undersamplingFactor,1-undersamplingFactor), replace = TRUE)
  )
  
  undersampledDataset <- dataset[selectionVector,]
  return (undersampledDataset)
}
# Output: Datensatz nach Undersampling


```

```{r Undersampling Example}

cat("Vor Undersampling: \n")
table(ccdata$Class)

cat("\n", "Nach Undersampling: \n")
undersampled_data <- undersample(0.99, ccdata)
table(undersampled_data$Class)

cat("\n","undersampling um ", round((1-(table(undersampled_data$Class)[1]/table(ccdata$Class)[1])),4)*100, " % \n")

```
Dabei wird mit der Sample Funktion gearbeitet, wodurch das tatsächliche Verhältnis minimal abweichen kann. Nach ersten Tests mit dem Undersampling, SMOTE und XGBoost wurde davon ausgegangen, dass selbst starkes Undersampling keinen negativen Einfluss auf die Performance haben würde. 

```{r impact of undersampling rate on performance, echo=FALSE}
hyperparameter_tuning_results_legacy_v1 <- read.csv("~/DAC/hyperparameter_tuning_v1.csv")[-1:-3]

plot(subset(hyperparameter_tuning_results_legacy_v1, as.logical((
  hyperparameter_tuning_results_legacy_v1$nRatioSMOTE > 0.4) * 
    (hyperparameter_tuning_results_legacy_v1$kSMOTE < 50) * 
    (hyperparameter_tuning_results_legacy_v1$kFold==4)
))[,c(2,5)])

```


### 4.2 Cross-Validation

Um die Beurteilung der Performance von Machine-Learning Algorithmen noch zuverlässlicher zu machen, kann anstatt einem einzigen Subsetting in Test- und Trainingsdatensatz auch Cross-Validation angewendet werden. Damit wird das Risiko minimiert, dass die Ergebnisse von der zufälligen Auswahl der Samples abhängen, die in den Trainings- bzw. Testdatensatz aufgenommen werden. Das kann dadurch erreicht werden, dass der gesamte Datensatz in k gleich große Teile unterteilt wird. In mehreren Durchläufen wird daraufhin je eins dieser k Teile als Testdatensatz und die restlichen Teile als Trainingsdatensatz genutzt. Dabei werden alle k Kombinationen durchlaufen. 

Bei der Implementierung und Umsetzung ist herausfordernd, dass die Cross-Validation durch das Durchlaufen der verschiedenen Kombinationen die Laufzeit im Vergleich zu einem einfachen Durchlauf erheblich erhöht. In so mehr Teile der Datensatz dabei unterteilt wird, desto mehr Kombinationen und damit längere Laufzeit entsteht. Daher wurde hier ein Wert von 4 gewählt, womit in jedem Durchlauf 75% des Datensatzes als Trainingsdaten und 25% als Testdaten genutzt werden. 

Bei der Implementierung müssen zunächst Funktionen zum Teilen des Datensatzes in k Teile und zum Zusammenfügen erstellt werden:

```{r Funktionen für Cross-Validation 1}


# Aufteilen des Datensatzes in k teile:

# Input: Anzahl in wie viele subsets geteilt werden soll, Datensatz
splitIntoKSubsets <- function(K = 4, dataset = ccdata){
  subsets <- list()
  # split von 1-k
  split <- sample(1:K, size = length(dataset[,1]), replace = TRUE)
  for (i in 1:K){
    subsets <- append(subsets, list(subset(dataset, split == i)))
  }
  return (subsets)
}
# Output: list mit den subsets


```

```{r Funktionen für Cross-Validation 2}
## Auswählen des x-ten Datensatzes als Testdatensatz, zusammenfassen der restlichen als Trainingsdatensatz

# Input: Index des Subsets welcher Testdatensatz sein soll, Liste mit Subsets
mergeSubsetsIntoTrainAndTest <- function(whichSubsetAsTest = 1, subsetList = splitIntoKSubsets()){
  test <- subsetList[[whichSubsetAsTest]]
  train <- data.frame()
  for (i in (1:length(subsetList))[-whichSubsetAsTest]){
    train <- rbind(train, subsetList[[i]])
  }
  return(list(
    "test" = test,
    "train" = train))
}
# Output: List mit train und test - Datensatz
```

Die übergeordnete Funktion für die Cross Validation ruft einige zuvor definierte Funktionen auf. Beim Aufrufen der Funktion muss u.a. als Parameter eingegeben werden, welcher Oversampling-Algorithmus und Classifier genutzt werden soll, wie stark die Majority Class undersampled werden soll, in wie viele Teile der Datensatz unterteilt werden soll (entspricht Anzahl der Kombinationen der Cross Validation) und wie das Verhältnis zwischen Minority und Majority Class nach Under- & Oversampling in den Trainingsdatensätzen sein soll. 

Bei der Betrachtung der Implementierung der Funktion "splitIntoKSubsets" fällt auf, dass das Verhältnis zwischen Minority und Majority Klasse nach dem Teilen nicht in jedem Subset gleich sein wird. Außerdem können die Trainigsdatensätze später noch undersampled werden. Darauf ergibt sich eine Herausforderung für das Setzen des Parameters N von SMOTE. Möchten wir nach dem SMOTEN von jedem dieser Trainingsdatensätze in diesem ein Verhältnis von z.B. 1:1 zwischen Majority und Minority Class haben, müssten wir den dafür benötigten N Wert für jedes Subset unter Berücksichtigung des Undersampling Faktors neu bestimmen. Daher wurde hier implementiert, dass für jedes Subset der für ein gewünschtes Zielverhältnis ("nDesiredRatioOfClassesForSMOTE") benötigte N Parameter für SMOTE ("nAbsoluteSMOTE") berechnet wird. 


```{r Cross-Validation}

##### K-fold Cross-Validation:
## Einmal Datensatz splitten
## schleife von 1:k, jedes mal ein anderes Subset als Testdatensatz wählen
## in der Schleife: Aktuellen Test und Train-Datensatz in Model geben & AOC speichern
## Nach durchlauf der Schlaufe: Durchschnitt von AOC bilden & returnen

# Input: gewünschte Algorithmen, Anzahl der gewünschten Subsets für Crossvalidation, Datensatz, Parameter für SMOTE
kFoldCrossValidate <- function(oversampling_algorithm, classifier, kForFold, entireDataset, undersampling_factor, kForSMOTE, nDesiredRatioOfClassesForSMOTE){
  subsets <- splitIntoKSubsets(K = kForFold, dataset = entireDataset)
  vectorOfAOCresults <- c()
  for (i in 1:length(subsets)){
    inputDataForModelRun <- mergeSubsetsIntoTrainAndTest(whichSubsetAsTest = i, subsetList = subsets)
    
    # UNDERSAMPLING : 
    #print(undersampling_factor)
    #print(table(inputDataForModelRun$train$Class))
    undersampled_train <- undersample(undersampling_factor, inputDataForModelRun$train)
    #print(table(inputDataForModelRun$train$Class))
    
    
    
    # da in jedem subset ein leicht anderes anderes Verhältnis von min zu maj
    # klasse vorliegt, sollte der N parameter für jedes subset bestimmt werden.
    # nDesiredRatioOfClassesForSMOTE von 100% (= 1) bedeutet dabei, 
    # dass nach SMOTE gleich viele minority wie 
    # majority samples im Datensatz sein sollen (100% bedeutet also gleich viele,
    # das sollte wieder einen absoluten n Wert von ca. 577 ergeben)
    # Das Hyperparametertuning erfolgt somit später mit prozentwerten statt
    # absoluten werten. z.B. 120% (=1.2) würde also bedeuten, wir wollen
    # 20% mehr minority als majority samples nach SMOTE
    nAbsoluteSMOTE <- table(undersampled_train$Class)[1]/table(undersampled_train$Class)[2]
    nAbsoluteSMOTE <- round((nAbsoluteSMOTE-1)*nDesiredRatioOfClassesForSMOTE)
    
    
    resultsOfSingleRun <- list()
    
    if (classifier == "xgboost"){
      if (oversampling_algorithm == "smote"){
        cat("run_smote_with_xgboost - with N = ",nAbsoluteSMOTE)
        resultsOfSingleRun <- run_smote_with_xgboost(kSMOTE = kForSMOTE, nSMOTE = nAbsoluteSMOTE, train = undersampled_train, test = inputDataForModelRun$test)
      }
      else if (oversampling_algorithm == "asn"){
        cat("run_asn_smote_with_xgboost - with N = ",nAbsoluteSMOTE)
        resultsOfSingleRun <- run_asn_smote_with_xgboost(kSMOTE = kForSMOTE, nSMOTE = nAbsoluteSMOTE,  train_undersampled = undersampled_train, train_not_undersampled = inputDataForModelRun$train, test = inputDataForModelRun$test)
      }
      else {cat("FEHLER - FEHLER - FEHLER - FEHLER - FEHLER")}
    }
    else if (classifier == "logistic_regression"){
      if (oversampling_algorithm == "smote"){
        cat("run_smote_with_logistic_regression - with N = ",nAbsoluteSMOTE)
        resultsOfSingleRun <- run_smote_with_logistic_regression(kSMOTE = kForSMOTE, nSMOTE = nAbsoluteSMOTE, train = undersampled_train, test = inputDataForModelRun$test)
      }
      else if (oversampling_algorithm == "asn"){
        cat("run_asn_smote_with_logistic_regression - with N = ",nAbsoluteSMOTE)
        resultsOfSingleRun <- run_asn_smote_with_logistic_regression(kSMOTE = kForSMOTE, nSMOTE = nAbsoluteSMOTE, train_undersampled = undersampled_train, train_not_undersampled = inputDataForModelRun$train, test = inputDataForModelRun$test)
      }
      else {cat("FEHLER - FEHLER - FEHLER - FEHLER - FEHLER")}
    }
    else {cat("FEHLER - FEHLER - FEHLER - FEHLER - FEHLER")}
    
    
    vectorOfAOCresults <- c(vectorOfAOCresults, resultsOfSingleRun$AUC)
  }

  return(list(
    "AUCResultsOfSingleRuns" = vectorOfAOCresults, 
    "AUCAverage" = mean(vectorOfAOCresults),
    "oversampling_algorithm" = oversampling_algorithm, 
    "classifier" = classifier
  ))
}
# Output: verwendete Algorithmen, List mit Durchschnitts-AUC, Vektor der AUCs der einzelnen Durchläufe und dem verwendeten Classifier und oversampling algorithmus

```

### 4.3 Hyperparameteroptimierung  

zu N: In ersten Tests mit SMOTE und XGBoost kaum Einfluss auf die Performance
(außer Extremwerte die so niedrig sind das de facto nicht mehr gesmotet wird -> dann nähert sich performance an die Performance ohne SMOTE an). Daher entscheidung nratiosmote nicht zu optimieren, das Verhältnis im Datensatz nach SMOTE/ASN soll also immer 1:1 sein. 

```{r impact of n on performance, echo=FALSE}
hyperparameter_tuning_results_legacy_v1 <- read.csv("~/DAC/hyperparameter_tuning_v1.csv")[-1:-3]

plot(subset(hyperparameter_tuning_results_legacy_v1, as.logical((
  hyperparameter_tuning_results_legacy_v1$undersamplingFactor == 0) * 
    (hyperparameter_tuning_results_legacy_v1$kSMOTE < 100) * 
    (hyperparameter_tuning_results_legacy_v1$kFold==4)
))[,c(4,5)])

```

Hyperp....


```{r Hyperparameteroptimierung}
# Input: zu nutzende Algorithmen, kFold (für crossvalidation), der Datensatz, der auszuprobierende Wertebereich
#       für k (SMOTE) und n (SMOTE, siehe auch Dokumentation von kFoldCrossValidate),
#       gewünschtes Undersampling (0 = kein Undersampling)
tryRandomHyperparameter <- function (oversampling_algorithm, classifier, kForFold, entireDataset, kForSMOTE, nDesiredRatioSMOTERange, undersamplingFactorRange){
  kToTest <- numeric()
  if (length(kForSMOTE)==1){
    kToTest <- kForSMOTE[1]
  } else {
    kToTest <- sample(kForSMOTE,1)
  }
  
  nRatioToTest <- numeric()
  if (length(nDesiredRatioSMOTERange)==1){
    nRatioToTest <- nDesiredRatioSMOTERange[1]
  } else {
    nRatioToTest <- sample(nDesiredRatioSMOTERange,1)
  }

  undersamplingFactorToTest <- numeric()
  if (length(undersamplingFactorRange)==1){
    undersamplingFactorToTest <- undersamplingFactorRange[1]
  } else {
    undersamplingFactorToTest <- sample(undersamplingFactorRange,1)
  }


  cat("---------------------------- getestete Parameter: nRatio=", nRatioToTest, ", kSMOTE=", kToTest,", undersamplingFactor=", undersamplingFactorToTest,  " ----------------------------")
  
  
  crossValidatedAUC <- kFoldCrossValidate(oversampling_algorithm, classifier, kForFold, entireDataset, undersamplingFactorToTest, kForSMOTE = kToTest, nDesiredRatioOfClassesForSMOTE = nRatioToTest)
  
  resultVector <- c(crossValidatedAUC$oversampling_algorithm, crossValidatedAUC$classifier, kForFold, undersamplingFactorToTest, kToTest, nRatioToTest, crossValidatedAUC$AUCAverage)
  return(resultVector)
}
# Output: Vector der Testergebnisse mit: genutzte algorithmen, kFold (für crossvalidation), 
#         kSMOTE, nRatioSMOTE und die AUC

```


### 4.4 Skript


### 4.5 Ergebnisse der Performance
```{r Performance des ASN-SMOTE}
#Code
```



## 5. Ausblick









Aufgabengebiete Philipp von Lovenberg:
Durcharbeiten SMOTE-Paper, Mitarbeit bei der Erstellung der Präsentationsfolien, Mitarbeit bei der Recherche nach modernen SMOTE Adaptionen, Erstellung der Cross-Validation, Hyperparametertuning von SMOTE-Algorithmus, Durchführen des Undersamplings, Mitarbeit bei der Erstellung des R Markdown Dokuments.

Aufgabengebiete Vincent Bläske:
Durcharbeiten SMOTE-Paper, Mitarbeit bei der Erstellung der Präsentationsfolien, Mitarbeit bei der Recherche nach modernen SMOTE Adaptionen, Implementierung des SMOTE-Algorithmus, Durcharbeiten des ASN-SMOTE Papers, Mitarbeit bei der R Implementierung des ASN-SMOTE, Auswahl und Implementierung der Classifier, Hyperparametertuning der Classifier, Plotten, Performanceauswertung, Mitarbeit bei der Erstellung des R Markdown Dokuments.

Aufgabengebiete Dennis Götz:
Durcharbeiten SMOTE-Paper, Mitarbeit bei der Erstellung der Präsentationsfolien, Mitarbeit bei der Recherche nach modernen SMOTE Adaptionen, Durcharbeiten des ASN-SMOTE Papers und der dazugehörigen Python Implementierung, Mitarbeit bei der R Implementierung des ASN-SMOTE und des Modells der logistischen Regression, Mitarbeit bei der Erstellung des R Markdown Dokuments.
